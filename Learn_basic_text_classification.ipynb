{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learn basic-text-classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic4_occAAiAT"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "ioaprt5q5US7"
      },
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "yCl0eTNH5RS3"
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# Text Classification with Movie Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/tf2_text_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/tf2_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/collections/nnlm/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub models</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "This notebook classifies movie reviews as *positive* or *negative* using the text of the review. This is an example of *binary*—or two-class—classification, an important and widely applicable kind of machine learning problem. \n",
        "\n",
        "We'll use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews. \n",
        "\n",
        "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow, and [TensorFlow Hub](https://www.tensorflow.org/hub), a library and platform for transfer learning. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uW5Q7enrQdo"
      },
      "source": [
        "Dalam notebookini dilakukan teks klasifikasi terhadap movie rating menggunakan binary terhadaap dua klasifikasi. Digunakan IMDB dataset dengean konten teks 50.000 movie reviews dalam Internet Movie Database. Data terbagi dengan 25.000 review dalam data training dan 25.000 review data testing. Data train dan data test diset seimbang dengan artian untuk angka positif dan negatif review.\n",
        "Penggunaan notebook dengan tf.keras dengan High level API untuk membangun model dalam TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrk8NjzhSBh-"
      },
      "source": [
        "### More models\n",
        "[Here](https://tfhub.dev/s?module-type=text-embedding) you can find more expressive or performant models that you could use to generate the text embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4DN769E2O_R"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ew7HTbPpCJH"
      },
      "source": [
        "#import library numpy dan tensorflow\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsKG535pHep"
      },
      "source": [
        "## Download the IMDB dataset\n",
        "\n",
        "The IMDB dataset is available on [TensorFlow datasets](https://github.com/tensorflow/datasets). The following code downloads the IMDB dataset to your machine (or the colab runtime):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dy1wvCSsvzc"
      },
      "source": [
        "Download train data dan test data dengan melakukan split database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXXx5Oc3pOmN"
      },
      "source": [
        "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], \n",
        "                                  batch_size=-1, as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = tfds.as_numpy(train_data)\n",
        "test_examples, test_labels = tfds.as_numpy(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l50X3GfjpU4r"
      },
      "source": [
        "## Explore the data \n",
        "\n",
        "Let's take a moment to understand the format of the data. Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_q0zA6xs4_e"
      },
      "source": [
        "Setiap contoh adalah kalimat yang mewakili ulasan film dan label yang sesuai. Kalimat tersebut tidak diproses sebelumnya. Label adalah nilai bilangan bulat dari 0 atau 1, di mana 0 adalah ulasan negatif, dan 1 adalah ulasan positif."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8qCnve_-lkO"
      },
      "source": [
        "print(\"Training entries: {}, test entries: {}\".format(len(train_examples), len(test_examples)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnKvHWW4-lkW"
      },
      "source": [
        "Let's print first 10 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtTS4kpEpjbi"
      },
      "source": [
        "train_examples[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFtaCHTdc-GY"
      },
      "source": [
        "Let's also print the first 10 labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvAjVXOWc6Mj"
      },
      "source": [
        "train_labels[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "The neural network is created by stacking layers—this requires three main architectural decisions:\n",
        "\n",
        "* How to represent the text?\n",
        "* How many layers to use in the model?\n",
        "* How many *hidden units* to use for each layer?\n",
        "\n",
        "In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n",
        "\n",
        "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have two advantages:\n",
        "*   we don't have to worry about text preprocessing,\n",
        "*   we can benefit from transfer learning.\n",
        "\n",
        "For this example we will use a model from [TensorFlow Hub](https://www.tensorflow.org/hub) called [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2).\n",
        "\n",
        "There are two other models to test for the sake of this tutorial:\n",
        "* [google/nnlm-en-dim50-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim50-with-normalization/2) - same as [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2), but with additional text normalization to remove punctuation. This can help to get better coverage of in-vocabulary embeddings for tokens on your input text.\n",
        "* [google/nnlm-en-dim128-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2) - A larger model with an embedding dimension of 128 instead of the smaller 50."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE-VscCBtPu0"
      },
      "source": [
        "Pada neural network dibentuk dalam penempukan layer dengan 3 kebutuhan arsitektur:\n",
        "1. Bagaimana kalimat merepresentasikan suatu hal?\n",
        "2. Berapa banyak layer yang digunakan untuk model?\n",
        "3. Berapa banyak unit tersembunyi dalam tiap layer?\n",
        "\n",
        "Dalam contoh ini, terdapat input data berupa kalimat. Label untuk memprediksi yaitu antara 1 dan 0. Salah satu cara untuk merepresentasikan teks adalah dengan mengubah kalimat menjadi vektor embeddings. Kita dapat menggunakan penyisipan teks yang telah dilatih sebelumnya sebagai lapisan pertama, yang akan memiliki dua keuntungan:\n",
        "\n",
        "1. kita tidak perlu khawatir tentang pra-pemrosesan teks,\n",
        "2. kita bisa mendapatkan keuntungan dari transfer belajar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In2nDpTLkgKa"
      },
      "source": [
        "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that the output shape of the produced embeddings is a expected: `(num_examples, embedding_dimension)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho_8RPuWuAzP"
      },
      "source": [
        "Pertama, buat layer keras dalam TensorFlow untuk pemodelan kalimat dan buat sebuah pasangan untuk contoh input. Catatan hasil bentuk yang dihasilkan(num_examples dan embeding_dimension) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NUbzVeYkgcO"
      },
      "source": [
        "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfSbV6igl1EH"
      },
      "source": [
        "Let's now build the full model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxd2yo3IucEP"
      },
      "source": [
        "Bentuk model dengan keras sequential, penambahan hub_layer, penambahan lapisan dense dalam 16 dengan penggunaan relu, lalu output diubah menjadi 1..\n",
        "Selanjutnya model disummary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpKOoWgu-llD"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PbKQ6mucuKL"
      },
      "source": [
        "The layers are stacked sequentially to build the classifier:\n",
        "\n",
        "1. The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The model that we are using ([google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`.\n",
        "2. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
        "3. The last layer is densely connected with a single output node. This outputs logits: the log-odds of the true class, according to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZPiYwQ5u4Dn"
      },
      "source": [
        "Lapisan ditumpuk secara berurutan untuk membangun pengklasifikasi:\n",
        "\n",
        "1. Lapisan pertama adalah lapisan TensorFlow Hub. Lapisan ini menggunakan Model Tersimpan yang telah dilatih sebelumnya untuk memetakan kalimat ke dalam vektor penyematannya. Model yang kami gunakan (google/nnlm-en-dim50/2) membagi kalimat menjadi token, menyematkan setiap token, lalu menggabungkan penyematan. Dimensi yang dihasilkan adalah: (num_examples, embedding_dimension).\n",
        "2. Vektor keluaran dengan panjang tetap ini disalurkan melalui lapisan yang terhubung penuh (Dense) dengan 16 unit tersembunyi.\n",
        "3. Lapisan terakhir terhubung erat dengan simpul keluaran tunggal. Ini menghasilkan logits: log-peluang dari kelas sebenarnya, menurut model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XMwnDOp-llH"
      },
      "source": [
        "### Hidden units\n",
        "\n",
        "The above model has two intermediate or \"hidden\" layers, between the input and output. The number of outputs (units, nodes, or neurons) is the dimension of the representational space for the layer. In other words, the amount of freedom the network is allowed when learning an internal representation.\n",
        "\n",
        "If a model has more hidden units (a higher-dimensional representation space), and/or more layers, then the network can learn more complex representations. However, it makes the network more computationally expensive and may lead to learning unwanted patterns—patterns that improve performance on training data but not on the test data. This is called *overfitting*, and we'll explore it later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jbx9pBuwUxz"
      },
      "source": [
        "**Hidden Units**\n",
        "Model di atas memiliki dua lapisan perantara atau \"tersembunyi\", antara input dan output. Jumlah output (unit, node, atau neuron) adalah dimensi ruang representasional untuk lapisan. Dengan kata lain, jumlah kebebasan jaringan diperbolehkan saat mempelajari representasi internal.\n",
        "\n",
        "Jika sebuah model memiliki lebih banyak unit tersembunyi (ruang representasi berdimensi lebih tinggi), dan/atau lebih banyak lapisan, maka jaringan dapat mempelajari representasi yang lebih kompleks. Namun, dalam membuat jaringan lebih berat secara komputasi dan dapat menyebabkan mempelajari pola yang tidak diinginkan dalam meningkatkan kinerja pada data pelatihan tetapi tidak pada data pengujian. Ini disebut overfitting, dan kita akan membahasnya nanti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### Loss function and optimizer\n",
        "\n",
        "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function. \n",
        "\n",
        "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
        "\n",
        "Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n",
        "\n",
        "Now, configure the model to use an optimizer and a loss function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA6NyUWww7l7"
      },
      "source": [
        "*Loss function and Optimizer* Sebuah model membutuhkan lossfunction dan pengoptimal untuk pelatihan. Karena ini adalah masalah klasifikasi biner dan model menghasilkan probabilitas (lapisan unit tunggal dengan aktivasi sigmoid), maka  digunakan fungsi loss binary_crossentropy.\n",
        "\n",
        "Ini bukan satu-satunya pilihan untuk fungsi kerugian, misalnya, memilih mean_squared_error. Tetapi, umumnya, binary_crossentropy lebih baik untuk menangani probabilitas—ini mengukur \"jarak\" antara distribusi probabilitas, atau dalam kasus kami, antara distribusi ground-truth dan prediksi.\n",
        "\n",
        "Kemudian, ketika  menjelajahi masalah regresi (misalnya, untuk memprediksi harga rumah) maka dilihat bagaimana menggunakan fungsi loss lain yang disebut mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr0GP-cQ-llN"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCWYwkug-llQ"
      },
      "source": [
        "## Create a validation set\n",
        "\n",
        "When training, we want to check the accuracy of the model on data it hasn't seen before. Create a *validation set* by setting apart 10,000 examples from the original training data. (Why not use the testing set now? Our goal is to develop and tune our model using only the training data, then use the test data just once to evaluate our accuracy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwB0VpiMxqzE"
      },
      "source": [
        "*Buat set validasi*\n",
        "Saat pelatihan, dilakukan pemeriksaan keakuratan model pada data yang belum pernah dilihat sebelumnya. Buat set validasi dengan memisahkan 10.000 contoh dari data pelatihan asli. (Mengapa tidak menggunakan set pengujian sekarang? Tujuan kami adalah mengembangkan dan menyempurnakan model kami hanya dengan menggunakan data pelatihan, kemudian menggunakan data uji sekali saja untuk mengevaluasi akurasi kami)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NpcXY9--llS"
      },
      "source": [
        "x_val = train_examples[:10000]\n",
        "partial_x_train = train_examples[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Train the model for 40 epochs in mini-batches of 512 samples. This is 40 iterations over all samples in the `x_train` and `y_train` tensors. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4AMwv13x1jR"
      },
      "source": [
        "Pelatihan model untuk 40 epocs dalam 512 sampel. 40 Iterasi dilakukan dengan pada semua x_train dan y_train tensor. Ketika melakukan traning, dilakukan pengawasan model terhadap akurasi 10.000 sampel dalam validasi set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXSGrjWZ-llW"
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzQo_XnWyJlW"
      },
      "source": [
        "Evaluasi model dilakukan untuk melihat bagaimana jangkauan model. Terdapat dua nilai yang direturn. Loss(angka yang merepresentasikan error, lebih kecil maka lebih bagus) dan akurasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "source": [
        "results = model.evaluate(test_examples, test_labels)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1iEXVTR0Z2t"
      },
      "source": [
        "This fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8znOQ-syhYZ"
      },
      "source": [
        "Pendekatan model akurasi didapatkan sekitar 87% dan dengan model pendekatan advanced dengan mendekati 95%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KggXVeL-llZ"
      },
      "source": [
        "## Create a graph of accuracy and loss over time\n",
        "\n",
        "`model.fit()` returns a `History` object that contains a dictionary with everything that happened during training:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxAgmnQ0y1fu"
      },
      "source": [
        "Model.fit() dikembalikan dalam sebuah history object terhadap contain dictionary terhadap seluruh apa yang terjadi dalam training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcvSXvhp-llb"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRKsqL40-lle"
      },
      "source": [
        "There are four entries: one for each monitored metric during training and validation. We can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WkaUb3YzCuj"
      },
      "source": [
        "Terdapat empat entri: satu untuk setiap metrik yang dipantau selama pelatihan dan validasi. Kita dapat menggunakan ini untuk merencanakan kehilangan pelatihan dan validasi untuk perbandingan, serta akurasi pelatihan dan validasi:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGoYf2Js-lle"
      },
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hXx-xOv-llh"
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFEmZ5zq-llk"
      },
      "source": [
        "In this plot, the dots represent the training loss and accuracy, and the solid lines are the validation loss and accuracy.\n",
        "\n",
        "Notice the training loss *decreases* with each epoch and the training accuracy *increases* with each epoch. This is expected when using a gradient descent optimization—it should minimize the desired quantity on every iteration.\n",
        "\n",
        "This isn't the case for the validation loss and accuracy—they seem to peak after about twenty epochs. This is an example of overfitting: the model performs better on the training data than it does on data it has never seen before. After this point, the model over-optimizes and learns representations *specific* to the training data that do not *generalize* to test data.\n",
        "\n",
        "For this particular case, we could prevent overfitting by simply stopping the training after twenty or so epochs. Later, you'll see how to do this automatically with a callback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy6RVp_gzLcy"
      },
      "source": [
        "Dalam plot ini, titik-titik mewakili kehilangan dan akurasi pelatihan, dan garis padat adalah kehilangan dan akurasi validasi.\n",
        "\n",
        "Perhatikan loss decresease dengan setiap epoks dan akurasi pelatihan meningkat dengan setiap epoks. Diharapkan saat menggunakan optimasi penurunan gradien—ini harus meminimalkan kuantitas yang diinginkan pada setiap iterasi.\n",
        "\n",
        "Ini tidak terjadi pada kehilangan validasi dan akurasi—mereka tampaknya mencapai puncaknya setelah sekitar dua puluh epoch. Ini adalah contoh overfitting: model berperforma lebih baik pada data pelatihan daripada pada data yang belum pernah dilihat sebelumnya. Setelah titik ini, model terlalu mengoptimalkan dan mempelajari representasi khusus untuk data pelatihan yang tidak digeneralisasi untuk data pengujian.\n",
        "\n",
        "Untuk kasus khusus ini, kita dapat mencegah overfitting hanya dengan menghentikan pelatihan setelah sekitar dua puluh epoch. Nanti, Anda akan melihat cara melakukannya secara otomatis dengan panggilan balik."
      ]
    }
  ]
}